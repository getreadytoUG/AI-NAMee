

Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:02<00:00,  1.20s/it]
WARNING: Resizing the embedding matrix to match the tokenizer vocab size.
--> Model /media/user/datadisk/LLM_models/llama-3.2-Korean-Bllossom-3B
--> /media/user/datadisk/LLM_models/llama-3.2-Korean-Bllossom-3B has 3212.752896 Million params
trainable params: 36,700,160 || all params: 3,249,453,056 || trainable%: 1.1294
--> Training Set Length = 1933
--> Validation Set Length = 2
length of dataset_train 1933
--> Num of Training Set Batches loaded = 966
--> Num of Validation Set Batches loaded = 1
--> Num of Validation Set Batches loaded = 1
Starting epoch 0/30
train_config.max_train_step: 0
/home/user/anaconda3/envs/llm-api/lib/python3.10/site-packages/torch/cuda/memory.py:343: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
  warnings.warn(

































































Training Epoch: 1/30, step 965/966 completed (loss: 0.0018621721537783742, lr: 0.0002): : 8it [02:10, 16.32s/it]
Max CUDA memory allocated was 8 GB
Max CUDA memory reserved was 10 GB
Peak active CUDA memory was 8 GB
CUDA Malloc retries : 0
CPU Total Peak Memory consumed during the train (max): 2 GB
 eval_ppl=tensor(1.2322, device='cuda:0') eval_epoch_loss=tensor(0.2088, device='cuda:0')
we are about to save the PEFT modules
evaluating Epoch: 100%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ[39m| 1/1 [00:00<00:00,  2.78it/s]
/home/user/anaconda3/envs/llm-api/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /media/user/datadisk/LLM_models/llama-3.2-Korean-Bllossom-3B - will assume that the vocabulary was not modified.
  warnings.warn(
Training Epoch: 2/30, step 2/966 completed (loss: 0.0015384164871647954, lr: 0.00017):
PEFT modules are saved in /home/user/eomjimin/AI_Namee/Fine-tuning/lora_output directory
best eval loss on epoch 1 is 0.20881657302379608
Epoch 1: train_perplexity=1.0027, train_epoch_loss=0.0027, epoch time 131.13009598999997s
Starting epoch 1/30


































































Training Epoch: 2/30, step 965/966 completed (loss: 0.0018670912832021713, lr: 0.00017): : 8it [02:14, 16.80s/it]
evaluating Epoch:   0%|[32m                                            [39m| 0/1 [00:00<?, ?it/s]
Max CUDA memory allocated was 8 GB
Max CUDA memory reserved was 11 GB
Peak active CUDA memory was 8 GB
CUDA Malloc retries : 0
CPU Total Peak Memory consumed during the train (max): 2 GB
 eval_ppl=tensor(1.1598, device='cuda:0') eval_epoch_loss=tensor(0.1482, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /home/user/eomjimin/AI_Namee/Fine-tuning/lora_output directory
best eval loss on epoch 2 is 0.1482258290052414
Epoch 2: train_perplexity=1.0014, train_epoch_loss=0.0014, epoch time 134.9050677080004s
Starting epoch 2/30
evaluating Epoch: 100%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ[39m| 1/1 [00:00<00:00,  2.57it/s]


































































Training Epoch: 3/30, step 965/966 completed (loss: 0.001015432528220117, lr: 0.00014450000000000002): : 8it [02:14, 16.86s/it]
evaluating Epoch:   0%|[32m                                            [39m| 0/1 [00:00<?, ?it/s]
Max CUDA memory allocated was 8 GB
Max CUDA memory reserved was 11 GB
Peak active CUDA memory was 8 GB
CUDA Malloc retries : 0
CPU Total Peak Memory consumed during the train (max): 2 GB
 eval_ppl=tensor(1.1667, device='cuda:0') eval_epoch_loss=tensor(0.1542, device='cuda:0')
Epoch 3: train_perplexity=1.0012, train_epoch_loss=0.0012, epoch time 135.35085822599922s
Starting epoch 3/30
evaluating Epoch: 100%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ[39m| 1/1 [00:00<00:00,  2.73it/s]

































































Training Epoch: 4/30, step 965/966 completed (loss: 0.001203338266350329, lr: 0.00012282500000000002): : 8it [02:12, 16.52s/it]
evaluating Epoch: 100%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ[39m| 1/1 [00:00<00:00,  2.71it/s]
Max CUDA memory allocated was 8 GB
Max CUDA memory reserved was 10 GB
Peak active CUDA memory was 8 GB
CUDA Malloc retries : 0
CPU Total Peak Memory consumed during the train (max): 2 GB
 eval_ppl=tensor(1.1429, device='cuda:0') eval_epoch_loss=tensor(0.1335, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /home/user/eomjimin/AI_Namee/Fine-tuning/lora_output directory
best eval loss on epoch 4 is 0.13353383541107178
Epoch 4: train_perplexity=1.0011, train_epoch_loss=0.0011, epoch time 132.66217348400005s
Starting epoch 4/30
train_config.max_train_step: 0

































































Training Epoch: 5/30, step 965/966 completed (loss: 0.0012308189179748297, lr: 0.00010440125000000001): : 8it [02:13, 16.65s/it]
evaluating Epoch: 100%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ[39m| 1/1 [00:00<00:00,  2.60it/s]
Max CUDA memory allocated was 8 GB
Max CUDA memory reserved was 11 GB
Peak active CUDA memory was 8 GB
CUDA Malloc retries : 0
CPU Total Peak Memory consumed during the train (max): 2 GB
 eval_ppl=tensor(1.1570, device='cuda:0') eval_epoch_loss=tensor(0.1458, device='cuda:0')
Epoch 5: train_perplexity=1.0011, train_epoch_loss=0.0011, epoch time 133.6545547550004s
Starting epoch 5/30
train_config.max_train_step: 0



































































Training Epoch: 6/30, step 965/966 completed (loss: 0.0008391995215788484, lr: 8.87410625e-05): : 8it [02:16, 17.11s/it]
evaluating Epoch: 100%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ[39m| 1/1 [00:00<00:00,  2.74it/s]
Max CUDA memory allocated was 8 GB
Max CUDA memory reserved was 9 GB
Peak active CUDA memory was 8 GB
CUDA Malloc retries : 0
CPU Total Peak Memory consumed during the train (max): 2 GB
 eval_ppl=tensor(1.1465, device='cuda:0') eval_epoch_loss=tensor(0.1367, device='cuda:0')
Epoch 6: train_perplexity=1.0011, train_epoch_loss=0.0011, epoch time 137.37215233500046s
Starting epoch 6/30
train_config.max_train_step: 0



































































Training Epoch: 7/30, step 965/966 completed (loss: 0.0008392763556912541, lr: 7.5429903125e-05): : 8it [02:15, 15.30s/it]
Max CUDA memory allocated was 8 GB
Max CUDA memory reserved was 9 GB
Peak active CUDA memory was 8 GB
CUDA Malloc retries : 0
Training Epoch: 7/30, step 965/966 completed (loss: 0.0008392763556912541, lr: 7.5429903125e-05): : 8it [02:15, 16.97s/it]
evaluating Epoch: 100%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ[39m| 1/1 [00:00<00:00,  2.58it/s]
Training Epoch: 8/30, step 1/966 completed (loss: 0.001145831891335547, lr: 6.41154176562
 eval_ppl=tensor(1.1547, device='cuda:0') eval_epoch_loss=tensor(0.1438, device='cuda:0')
Epoch 7: train_perplexity=1.0011, train_epoch_loss=0.0011, epoch time 136.2278618749997s
Starting epoch 7/30


































































Training Epoch: 8/30, step 965/966 completed (loss: 0.0008024488925002515, lr: 6.411541765624999e-05): : 8it [02:12, 14.97s/it]
Max CUDA memory allocated was 8 GB
Max CUDA memory reserved was 9 GB
Peak active CUDA memory was 8 GB
CUDA Malloc retries : 0
Training Epoch: 8/30, step 965/966 completed (loss: 0.0008024488925002515, lr: 6.411541765624999e-05): : 8it [02:12, 16.60s/it]
evaluating Epoch: 100%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ[39m| 1/1 [00:00<00:00,  2.66it/s]
Training Epoch: 9/30, step 1/966 completed (loss: 0.0010381224565207958, lr: 5.4498105007
 eval_ppl=tensor(1.1515, device='cuda:0') eval_epoch_loss=tensor(0.1411, device='cuda:0')
Epoch 8: train_perplexity=1.0010, train_epoch_loss=0.0010, epoch time 133.31758037000054s
Starting epoch 8/30


































































Training Epoch: 9/30, step 965/966 completed (loss: 0.0007974123582243919, lr: 5.449810500781249e-05): : 8it [02:14, 16.76s/it]
evaluating Epoch: 100%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ[39m| 1/1 [00:00<00:00,  2.62it/s]
Max CUDA memory allocated was 8 GB
Max CUDA memory reserved was 10 GB
Peak active CUDA memory was 8 GB
CUDA Malloc retries : 0
CPU Total Peak Memory consumed during the train (max): 2 GB
 eval_ppl=tensor(1.1553, device='cuda:0') eval_epoch_loss=tensor(0.1443, device='cuda:0')
Epoch 9: train_perplexity=1.0010, train_epoch_loss=0.0010, epoch time 134.58832619299938s
Starting epoch 9/30
train_config.max_train_step: 0


































































Training Epoch: 10/30, step 965/966 completed (loss: 0.0010776686249300838, lr: 4.6323389256640616e-05): : 8it [02:13, 16.74s/it]
evaluating Epoch: 100%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ[39m| 1/1 [00:00<00:00,  2.78it/s]
Training Epoch: 11/30, step 2/966 completed (loss: 0.0009736220818012953, lr: 3.937488086
Max CUDA memory allocated was 8 GB
Max CUDA memory reserved was 10 GB
Peak active CUDA memory was 8 GB
CUDA Malloc retries : 0
CPU Total Peak Memory consumed during the train (max): 2 GB
 eval_ppl=tensor(1.1489, device='cuda:0') eval_epoch_loss=tensor(0.1388, device='cuda:0')
Epoch 10: train_perplexity=1.0010, train_epoch_loss=0.0010, epoch time 134.38115645400012s
Starting epoch 10/30


































































Training Epoch: 11/30, step 965/966 completed (loss: 0.0010921289213001728, lr: 3.9374880868144525e-05): : 8it [02:13, 16.72s/it]
evaluating Epoch: 100%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ[39m| 1/1 [00:00<00:00,  2.56it/s]
Max CUDA memory allocated was 8 GB
Max CUDA memory reserved was 11 GB
Peak active CUDA memory was 8 GB
CUDA Malloc retries : 0
CPU Total Peak Memory consumed during the train (max): 2 GB
 eval_ppl=tensor(1.1593, device='cuda:0') eval_epoch_loss=tensor(0.1478, device='cuda:0')
Epoch 11: train_perplexity=1.0010, train_epoch_loss=0.0010, epoch time 134.245425737s
Starting epoch 11/30
train_config.max_train_step: 0

































































Training Epoch: 12/30, step 965/966 completed (loss: 0.000829757540486753, lr: 3.346864873792284e-05): : 8it [02:12, 16.62s/it]
evaluating Epoch: 100%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ[39m| 1/1 [00:00<00:00,  2.76it/s]
Max CUDA memory allocated was 8 GB
Max CUDA memory reserved was 11 GB
Peak active CUDA memory was 8 GB
CUDA Malloc retries : 0
CPU Total Peak Memory consumed during the train (max): 2 GB
Training Epoch: 13/30, step 7/966 completed (loss: 0.0013258906546980143, lr: 2.844835142
 eval_ppl=tensor(1.1383, device='cuda:0') eval_epoch_loss=tensor(0.1296, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /home/user/eomjimin/AI_Namee/Fine-tuning/lora_output directory
best eval loss on epoch 12 is 0.1295711100101471
Epoch 12: train_perplexity=1.0010, train_epoch_loss=0.0010, epoch time 133.4623654269999s
Starting epoch 12/30




































































Training Epoch: 13/30, step 964/966 completed (loss: 0.0008521865820512176, lr: 2.8448351
Max CUDA memory allocated was 8 GB
Max CUDA memory reserved was 11 GB
Peak active CUDA memory was 8 GB
CUDA Malloc retries : 0
CPU Total Peak Memory consumed during the train (max): 2 GB
 eval_ppl=tensor(1.1438, device='cuda:0') eval_epoch_loss=tensor(0.1343, device='cuda:0')
Epoch 13: train_perplexity=1.0010, train_epoch_loss=0.0010, epoch time 138.55047052699956s
Starting epoch 13/30
Training Epoch: 13/30, step 965/966 completed (loss: 0.0008079152903519571, lr: 2.8448351427234416e-05): : 8it [02:18, 17.26s/it]
evaluating Epoch: 100%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ[39m| 1/1 [00:00<00:00,  2.76it/s]


































































Training Epoch: 14/30, step 952/966 completed (loss: 0.0009385672165080905, lr: 2.4181098
Max CUDA memory allocated was 8 GB
Max CUDA memory reserved was 10 GB
Peak active CUDA memory was 8 GB
CUDA Malloc retries : 0

Training Epoch: 14/30, step 965/966 completed (loss: 0.0012658386258408427, lr: 2.4181098713149252e-05): : 8it [02:14, 16.78s/it]
 eval_ppl=tensor(1.1413, device='cuda:0') eval_epoch_loss=tensor(0.1322, device='cuda:0')
Epoch 14: train_perplexity=1.0010, train_epoch_loss=0.0010, epoch time 134.7191099620004s
Starting epoch 14/30
train_config.max_train_step: 0
evaluating Epoch: 100%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ[39m| 1/1 [00:00<00:00,  2.65it/s]


































































Training Epoch: 15/30, step 961/966 completed (loss: 0.0008331169956363738, lr: 2.0553933
Max CUDA memory allocated was 8 GB
Max CUDA memory reserved was 10 GB
Peak active CUDA memory was 8 GB
CUDA Malloc retries : 0
CPU Total Peak Memory consumed during the train (max): 2 GB
 eval_ppl=tensor(1.1431, device='cuda:0') eval_epoch_loss=tensor(0.1338, device='cuda:0')
Epoch 15: train_perplexity=1.0010, train_epoch_loss=0.0010, epoch time 134.3187753049997s
Starting epoch 15/30
Training Epoch: 15/30, step 965/966 completed (loss: 0.0008385344408452511, lr: 2.0553933906176864e-05): : 8it [02:13, 16.73s/it]
evaluating Epoch: 100%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ[39m| 1/1 [00:00<00:00,  2.73it/s]



































































Training Epoch: 16/30, step 965/966 completed (loss: 0.001080866320990026, lr: 1.7470843820250334e-05): : 8it [02:14, 15.20s/it]
Max CUDA memory allocated was 8 GB
Max CUDA memory reserved was 9 GB
Peak active CUDA memory was 8 GB
CUDA Malloc retries : 0
CPU Total Peak Memory consumed during the train (max): 2 GB
 eval_ppl=tensor(1.1440, device='cuda:0') eval_epoch_loss=tensor(0.1345, device='cuda:0')
Epoch 16: train_perplexity=1.0010, train_epoch_loss=0.0010, epoch time 135.11020244199972s
Starting epoch 16/30
Training Epoch: 16/30, step 965/966 completed (loss: 0.001080866320990026, lr: 1.7470843820250334e-05): : 8it [02:14, 16.83s/it]
evaluating Epoch: 100%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ[39m| 1/1 [00:00<00:00,  2.58it/s]


































































Training Epoch: 17/30, step 965/966 completed (loss: 0.0011034527560696006, lr: 1.4850217247212783e-05): : 8it [02:12, 16.57s/it]
Max CUDA memory allocated was 8 GB
Max CUDA memory reserved was 10 GB
Peak active CUDA memory was 8 GB
CUDA Malloc retries : 0
CPU Total Peak Memory consumed during the train (max): 2 GB
 eval_ppl=tensor(1.1459, device='cuda:0') eval_epoch_loss=tensor(0.1362, device='cuda:0')
Epoch 17: train_perplexity=1.0010, train_epoch_loss=0.0010, epoch time 133.0450399299998s
Starting epoch 17/30
train_config.max_train_step: 0
evaluating Epoch: 100%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ[39m| 1/1 [00:00<00:00,  2.78it/s]


































































Training Epoch: 18/30, step 962/966 completed (loss: 0.0010427002562209964, lr: 1.2622684
Max CUDA memory allocated was 8 GB
Max CUDA memory reserved was 10 GB
Peak active CUDA memory was 8 GB
CUDA Malloc retries : 0
CPU Total Peak Memory consumed during the train (max): 2 GB
 eval_ppl=tensor(1.1466, device='cuda:0') eval_epoch_loss=tensor(0.1368, device='cuda:0')
Epoch 18: train_perplexity=1.0009, train_epoch_loss=0.0009, epoch time 133.78328274700016s
Starting epoch 18/30
Training Epoch: 18/30, step 965/966 completed (loss: 0.0010828919475898147, lr: 1.2622684660130865e-05): : 8it [02:13, 16.66s/it]
evaluating Epoch: 100%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ[39m| 1/1 [00:00<00:00,  2.64it/s]


































































Training Epoch: 19/30, step 965/966 completed (loss: 0.0010333850514143705, lr: 1.0729281961111235e-05): : 8it [02:14, 16.76s/it]
evaluating Epoch:   0%|[32m                                            [39m| 0/1 [00:00<?, ?it/s]
Max CUDA memory allocated was 8 GB
Max CUDA memory reserved was 8 GB
Peak active CUDA memory was 8 GB
CUDA Malloc retries : 0
CPU Total Peak Memory consumed during the train (max): 2 GB
 eval_ppl=tensor(1.1454, device='cuda:0') eval_epoch_loss=tensor(0.1358, device='cuda:0')
Epoch 19: train_perplexity=1.0009, train_epoch_loss=0.0009, epoch time 134.56955330500023s
Starting epoch 19/30
evaluating Epoch: 100%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ[39m| 1/1 [00:00<00:00,  2.67it/s]
































































Training Epoch: 20/30, step 965/966 completed (loss: 0.0009895596886053681, lr: 9.11988966694455e-06): : 8it [02:10, 16.35s/it]
evaluating Epoch: 100%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ[39m| 1/1 [00:00<00:00,  2.66it/s]
Max CUDA memory allocated was 8 GB
Max CUDA memory reserved was 9 GB
Peak active CUDA memory was 8 GB
CUDA Malloc retries : 0
CPU Total Peak Memory consumed during the train (max): 2 GB
 eval_ppl=tensor(1.1460, device='cuda:0') eval_epoch_loss=tensor(0.1363, device='cuda:0')
Epoch 20: train_perplexity=1.0009, train_epoch_loss=0.0009, epoch time 131.2649263429994s
Starting epoch 20/30
train_config.max_train_step: 0


































































Training Epoch: 21/30, step 962/966 completed (loss: 0.0010163448750972748, lr: 7.7519062
Max CUDA memory allocated was 8 GB
Max CUDA memory reserved was 11 GB
Peak active CUDA memory was 8 GB
CUDA Malloc retries : 0
Training Epoch: 21/30, step 965/966 completed (loss: 0.0009890554938465357, lr: 7.751906216902867e-06): : 8it [02:13, 16.75s/it]
evaluating Epoch: 100%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ[39m| 1/1 [00:00<00:00,  2.65it/s]
Training Epoch: 22:   0%|[34m                                          [39m| 0/7 [00:00<?, ?it/s]
 eval_ppl=tensor(1.1482, device='cuda:0') eval_epoch_loss=tensor(0.1382, device='cuda:0')
Epoch 21: train_perplexity=1.0009, train_epoch_loss=0.0009, epoch time 134.46861261399954s
Starting epoch 21/30



































































Training Epoch: 22/30, step 961/966 completed (loss: 0.0007966452394612134, lr: 6.5891202
Max CUDA memory allocated was 8 GB
Max CUDA memory reserved was 10 GB
Peak active CUDA memory was 8 GB
CUDA Malloc retries : 0
Training Epoch: 22/30, step 965/966 completed (loss: 0.0008304010261781514, lr: 6.589120284367437e-06): : 8it [02:15, 16.89s/it]
evaluating Epoch: 100%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ[39m| 1/1 [00:00<00:00,  2.83it/s]
Training Epoch: 23:   0%|[34m                                          [39m| 0/7 [00:00<?, ?it/s]
 eval_ppl=tensor(1.1485, device='cuda:0') eval_epoch_loss=tensor(0.1385, device='cuda:0')
Epoch 22: train_perplexity=1.0009, train_epoch_loss=0.0009, epoch time 135.62840530500034s
Starting epoch 22/30


































































Training Epoch: 23/30, step 965/966 completed (loss: 0.0008304748334921896, lr: 5.600752241712321e-06): : 8it [02:13, 16.73s/it]
evaluating Epoch: 100%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ[39m| 1/1 [00:00<00:00,  3.10it/s]
Max CUDA memory allocated was 8 GB
Max CUDA memory reserved was 9 GB
Peak active CUDA memory was 8 GB
CUDA Malloc retries : 0
CPU Total Peak Memory consumed during the train (max): 2 GB
 eval_ppl=tensor(1.1499, device='cuda:0') eval_epoch_loss=tensor(0.1396, device='cuda:0')
Epoch 23: train_perplexity=1.0009, train_epoch_loss=0.0009, epoch time 134.2917441539994s
Starting epoch 23/30
evaluating Epoch: 100%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ[39m| 1/1 [00:00<00:00,  2.69it/s]






























































Training Epoch: 24/30, step 965/966 completed (loss: 0.0010255008237436414, lr: 4.760639405455473e-06): : 8it [02:06, 15.84s/it]
evaluating Epoch:   0%|[32m                                            [39m| 0/1 [00:00<?, ?it/s]
Max CUDA memory allocated was 8 GB
Max CUDA memory reserved was 11 GB
Peak active CUDA memory was 8 GB
CUDA Malloc retries : 0
CPU Total Peak Memory consumed during the train (max): 2 GB
 eval_ppl=tensor(1.1464, device='cuda:0') eval_epoch_loss=tensor(0.1367, device='cuda:0')
Epoch 24: train_perplexity=1.0009, train_epoch_loss=0.0009, epoch time 127.20613803800006s
Starting epoch 24/30
evaluating Epoch: 100%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ[39m| 1/1 [00:00<00:00,  2.60it/s]

































































Training Epoch: 25/30, step 965/966 completed (loss: 0.0008466138388030231, lr: 4.0465434946371515e-06): : 8it [02:12, 16.53s/it]
evaluating Epoch: 100%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ[39m| 1/1 [00:00<00:00,  2.55it/s]
Training Epoch: 26:   0%|[34m                                          [39m| 0/7 [00:00<?, ?it/s]
Max CUDA memory allocated was 8 GB
Max CUDA memory reserved was 10 GB
Peak active CUDA memory was 8 GB
CUDA Malloc retries : 0
CPU Total Peak Memory consumed during the train (max): 2 GB
 eval_ppl=tensor(1.1477, device='cuda:0') eval_epoch_loss=tensor(0.1378, device='cuda:0')
Epoch 25: train_perplexity=1.0009, train_epoch_loss=0.0009, epoch time 132.72652507599923s
Starting epoch 25/30




































































Training Epoch: 26/30, step 965/966 completed (loss: 0.0009156512096524239, lr: 3.4395619704415786e-06): : 8it [02:17, 17.23s/it]
evaluating Epoch:   0%|[32m                                            [39m| 0/1 [00:00<?, ?it/s]
Max CUDA memory allocated was 8 GB
Max CUDA memory reserved was 10 GB
Peak active CUDA memory was 8 GB
CUDA Malloc retries : 0
CPU Total Peak Memory consumed during the train (max): 2 GB
 eval_ppl=tensor(1.1515, device='cuda:0') eval_epoch_loss=tensor(0.1411, device='cuda:0')
Epoch 26: train_perplexity=1.0009, train_epoch_loss=0.0009, epoch time 138.30665549700097s
Starting epoch 26/30
evaluating Epoch: 100%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ[39m| 1/1 [00:00<00:00,  2.63it/s]





















































Training Epoch: 27/30
Max CUDA memory allocated was 8 GB
Max CUDA memory reserved was 9 GB
Peak active CUDA memory was 8 GB
CUDA Malloc retries : 0
Training Epoch: 27/30, step 965/966 completed (loss: 0.0008490120526403189, lr: 2.9236276748753417e-06): : 8it [02:15, 16.99s/it]
evaluating Epoch: 100
Training Epoch: 28:
 eval_ppl=tensor(1.1497, device='cuda:0') eval_epoch_loss=tensor(0.1395, device='cuda:0')
Epoch 27: train_perplexity=1.0009, train_epoch_loss=0.0009, epoch time 136.37947492999956s
Starting epoch 27/30

Training Epoch: 28/30, step 965/966 completed (loss: 0.0009434461244381964, lr: 2.4850835236440404e-06): : 8it [02:15, 16.94s/it]
evaluating Epoch:   0
Max CUDA memory allocated was 8 GB
Max CUDA memory reserved was 10 GB
Peak active CUDA memory was 8 GB
CUDA Malloc retries : 0
CPU Total Peak Memory consumed during the train (max): 2 GB
 eval_ppl=tensor(1.1473, device='cuda:0') eval_epoch_loss=tensor(0.1374, device='cuda:0')
Epoch 28: train_perplexity=1.0009, train_epoch_loss=0.0009, epoch time 136.0284950940004s
Starting epoch 28/30
evaluating Epoch: 100








































Training Epoch: 29/30, step 965/966 completed (loss: 0.0008061928092502058, lr: 2.1123209950974343e-06): : 8it [02:15, 15.24s/it]
Max CUDA memory allocated was 8 GB
Max CUDA memory reserved was 10 GB
Peak active CUDA memory was 8 GB
CUDA Malloc retries : 0
Training Epoch: 29/30, step 965/966 completed (loss: 0.0008061928092502058, lr: 2.1123209950974343e-06): : 8it [02:15, 16.98s/it]
evaluating Epoch: 100%|[32mâ–ˆ[39m| 1/1 [00:00<00:00,  2
Training Epoch: 30/30, step 1/966 completed (l
 eval_ppl=tensor(1.1495, device='cuda:0') eval_epoch_loss=tensor(0.1393, device='cuda:0')
Epoch 29: train_perplexity=1.0009, train_epoch_loss=0.0009, epoch time 136.2917084740002s
Starting epoch 29/30





































































Training Epoch: 30/30, step 965/966 completed (loss: 0.0008043753914535046, lr: 1.7954728458328192e-06): : 8it [02:19, 17.43s/it]
evaluating Epoch: 100%|[32mâ–ˆ[39m| 1/1 [00:00<00:00,  2
Max CUDA memory allocated was 8 GB
Max CUDA memory reserved was 10 GB
Peak active CUDA memory was 8 GB
CUDA Malloc retries : 0
CPU Total Peak Memory consumed during the train (max): 2 GB
 eval_ppl=tensor(1.1506, device='cuda:0') eval_epoch_loss=tensor(0.1403, device='cuda:0')
Epoch 30: train_perplexity=1.0009, train_epoch_loss=0.0009, epoch time 139.94249603499884s
Key: avg_train_prep, Value: 1.001059623559316
Key: avg_train_loss, Value: 0.001059008715674281
Key: avg_eval_prep, Value: 1.1519501209259033
Key: avg_eval_loss, Value: 0.1345130001505216
Key: avg_epoch_time, Value: 134.59764114439992
Key: avg_checkpoint_time, Value: 0.02172447370003283